# PIPELINE DEFINITION
# Name: mlops-mle-template
# Inputs:
#    labels: dict
#    location: str
#    model_name: str
#    name_bucket: str
#    output_bq_dataset: str
#    output_bq_project: str
#    output_bq_table: str
#    path_bucket: str
#    prod_config: dict
#    project: str
components:
  comp-batch-predict:
    executorLabel: exec-batch-predict
    inputDefinitions:
      artifacts:
        data_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        features:
          parameterType: LIST
        labels:
          parameterType: STRUCT
        location:
          parameterType: STRING
        model_name:
          parameterType: STRING
        name_bucket:
          parameterType: STRING
        output_bq_dataset:
          parameterType: STRING
        output_bq_project:
          parameterType: STRING
        output_bq_table:
          parameterType: STRING
        path_bucket:
          parameterType: STRING
        prod_config:
          parameterType: STRUCT
        project:
          parameterType: STRING
        target:
          parameterType: STRING
        train_data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        data_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        job_name:
          parameterType: STRING
  comp-exit-handler-1:
    dag:
      tasks:
        batch-predict:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-batch-predict
          dependentTasks:
          - get-data
          inputs:
            artifacts:
              data_input:
                taskOutputArtifact:
                  outputArtifactKey: dataset
                  producerTask: get-data
            parameters:
              features:
                taskOutputParameter:
                  outputParameterKey: features
                  producerTask: get-data
              labels:
                componentInputParameter: pipelinechannel--labels
              location:
                componentInputParameter: pipelinechannel--location
              model_name:
                componentInputParameter: pipelinechannel--model_name
              name_bucket:
                componentInputParameter: pipelinechannel--name_bucket
              output_bq_dataset:
                componentInputParameter: pipelinechannel--output_bq_dataset
              output_bq_project:
                componentInputParameter: pipelinechannel--output_bq_project
              output_bq_table:
                componentInputParameter: pipelinechannel--output_bq_table
              path_bucket:
                componentInputParameter: pipelinechannel--path_bucket
              prod_config:
                componentInputParameter: pipelinechannel--prod_config
              project:
                componentInputParameter: pipelinechannel--project
              target:
                taskOutputParameter:
                  outputParameterKey: target
                  producerTask: get-data
              train_data_path:
                taskOutputParameter:
                  outputParameterKey: train_data_path
                  producerTask: get-data
          taskInfo:
            name: Batch predictions
        get-data:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-data
          inputs:
            parameters:
              name_bucket:
                componentInputParameter: pipelinechannel--name_bucket
              path_bucket:
                componentInputParameter: pipelinechannel--path_bucket
          taskInfo:
            name: Get Data
        save-stats:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-save-stats
          dependentTasks:
          - batch-predict
          inputs:
            parameters:
              job_name:
                taskOutputParameter:
                  outputParameterKey: job_name
                  producerTask: batch-predict
              name_bucket:
                componentInputParameter: pipelinechannel--name_bucket
              path_bucket:
                componentInputParameter: pipelinechannel--path_bucket
          taskInfo:
            name: Save Stats Predictions
    inputDefinitions:
      parameters:
        pipelinechannel--labels:
          parameterType: STRUCT
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--name_bucket:
          parameterType: STRING
        pipelinechannel--output_bq_dataset:
          parameterType: STRING
        pipelinechannel--output_bq_project:
          parameterType: STRING
        pipelinechannel--output_bq_table:
          parameterType: STRING
        pipelinechannel--path_bucket:
          parameterType: STRING
        pipelinechannel--prod_config:
          parameterType: STRUCT
        pipelinechannel--project:
          parameterType: STRING
  comp-get-data:
    executorLabel: exec-get-data
    inputDefinitions:
      parameters:
        name_bucket:
          parameterType: STRING
        path_bucket:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        features:
          parameterType: LIST
        target:
          parameterType: STRING
        train_data_path:
          parameterType: STRING
  comp-save-stats:
    executorLabel: exec-save-stats
    inputDefinitions:
      parameters:
        job_name:
          parameterType: STRING
        name_bucket:
          parameterType: STRING
        path_bucket:
          parameterType: STRING
  comp-vertex-pipelines-notification-email:
    executorLabel: exec-vertex-pipelines-notification-email
    inputDefinitions:
      parameters:
        pipeline_task_final_status:
          isOptional: true
          parameterType: TASK_FINAL_STATUS
        recipients:
          description: A list of email addresses to send a notification to.
          parameterType: LIST
deploymentSpec:
  executors:
    exec-batch-predict:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - batch_predict
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef batch_predict(project           : str,\n                  location\
          \          : str,\n                  name_bucket       : str,\n        \
          \          path_bucket       : str,\n                  labels          \
          \  : Dict,\n                  prod_config       : Dict,\n              \
          \    model_name        : str,\n                  features          : list,\n\
          \                  target            : str,\n                  train_data_path\
          \   : str,\n                  data_input        : InputPath(\"Dataset\"\
          ), \n                  output_bq_project : str,\n                  output_bq_dataset\
          \ : str,\n                  output_bq_table   : str,\n                 \
          \ data_output       : OutputPath(\"Dataset\")) -> NamedTuple(\"output\"\
          , [(\"job_name\", str)]):\n\n    #==== Read input data from GCS ====#\n\
          \    from CustomLib.gcp import cloudstorage, bigquery\n    df = cloudstorage.read_csv_as_df(gcs_path\
          \ = data_input + '.csv')\n\n    #==== Get the model ====#\n    from pipeline.prod_modules\
          \ import get_model_by_display_name\n    model = get_model_by_display_name(display_name\
          \ = model_name, \n                                      project      = project,\
          \ \n                                      location     = location)\n\n \
          \   #==== Generate batch predictions ====#\n    from pipeline.prod_modules\
          \ import last_slash_detec, batch_prediction_request\n    df_to_predict,\
          \ job_name = batch_prediction_request(project           = project,\n   \
          \                                                    location          =\
          \ location,\n                                                       model_name\
          \        = model.name,\n                                               \
          \        input_path        = last_slash_detec(data_input),\n           \
          \                                            output_path       = last_slash_detec(data_output),\n\
          \                                                       labels         \
          \   = labels,\n                                                       prod_config\
          \       = prod_config,\n                                               \
          \        training_dataset  = train_data_path,\n                        \
          \                               features          = features,\n        \
          \                                               target            = target,\n\
          \                                                       name_bucket    \
          \   = name_bucket,\n                                                   \
          \    path_bucket       = path_bucket,\n                                \
          \                       df_to_predict     = df)\n\n    #==== Save predictions\
          \ ====#\n    # cloudstorage.write_csv(df       = df_to_predict, \n    #\
          \                        gcs_path = data_output +'.csv')\n    bigquery.write_df(df\
          \         = df_to_predict,\n                      project_id = output_bq_project,\
          \ \n                      dataset_id = output_bq_dataset, \n           \
          \           table_id   = output_bq_table,\n                      if_exists\
          \  = 'replace')\n\n    Output = NamedTuple(\"output\", [(\"job_name\", str)])\
          \  # Define the NamedTuple\n    result = Output(job_name=job_name)  # Create\
          \ an instance with the job_name\n\n    return result\n\n"
        image: us-central1-docker.pkg.dev/phrasal-talon-440215-i4/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-get-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_data(name_bucket : str,\n             path_bucket : str,\n\
          \             dataset         : OutputPath(\"Dataset\")) -> NamedTuple(\"\
          output\", [(\"features\", list),(\"target\", str),(\"train_data_path\",str)]):\n\
          \n    #==== Importing necessary libraries ====#\n    import pandas as pd\n\
          \    from sklearn.datasets import load_iris\n\n    #==== Loading the Iris\
          \ dataset ====#\n    iris = load_iris()\n    data = pd.DataFrame(data=iris['data'],\
          \ columns=iris['feature_names'])\n    data['target'] = iris['target']\n\n\
          \    # Define a mapping dictionary for the columns\n    column_mapping =\
          \ {\n        'sepal length (cm)': 'sepal_length_cm',\n        'sepal width\
          \ (cm)': 'sepal_width_cm',\n        'petal length (cm)': 'petal_length_cm',\n\
          \        'petal width (cm)': 'petal_width_cm',\n    }\n\n    # Rename the\
          \ columns using the mapping\n    data.rename(columns=column_mapping, inplace=True)\n\
          \n    columns = list(data.columns)\n    features = columns[:-1]\n    target\
          \ = columns[-1]\n\n    #==== Getting a random sample of 100 rows ====#\n\
          \    # If the dataset contains fewer than 100 rows, this will return all\
          \ rows in random order\n    random_sample = data.sample(n=min(100, len(data)),\
          \ random_state=42) # The random_state ensures reproducibility\n    data\
          \ = random_sample.drop(columns=['target'])\n\n    #==== Get the training_dataset\
          \ ====#\n    from CustomLib.gcp import cloudstorage\n    gcs_path = f\"\
          gs://{name_bucket}/{path_bucket}/last_training_path.txt\"\n    train_data_path\
          \ = cloudstorage.read_txt_as_str(gcs_path=gcs_path)\n\n    #==== Save the\
          \ df in GCS ====#\n    cloudstorage.write_csv(df       = data, \n      \
          \                     gcs_path = dataset + '.csv')\n\n    return features,\
          \ target, train_data_path\n\n"
        image: us-central1-docker.pkg.dev/phrasal-talon-440215-i4/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-save-stats:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - save_stats
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef save_stats(name_bucket      : str,\n               path_bucket\
          \      : str,\n               job_name         : str):\n\n    # Move stats\
          \ files in GCS\n    from pipeline.prod_modules import move_stats_file\n\
          \    move_stats_file(job_name    = job_name, \n                    name_bucket\
          \ = name_bucket, \n                    path_bucket = path_bucket)\n\n"
        image: us-central1-docker.pkg.dev/phrasal-talon-440215-i4/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-vertex-pipelines-notification-email:
      container:
        args:
        - --type
        - VertexNotificationEmail
        - --payload
        - ''
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.vertex_notification_email.executor
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.3.0
pipelineInfo:
  name: mlops-mle-template
root:
  dag:
    tasks:
      exit-handler-1:
        componentRef:
          name: comp-exit-handler-1
        inputs:
          parameters:
            pipelinechannel--labels:
              componentInputParameter: labels
            pipelinechannel--location:
              componentInputParameter: location
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--name_bucket:
              componentInputParameter: name_bucket
            pipelinechannel--output_bq_dataset:
              componentInputParameter: output_bq_dataset
            pipelinechannel--output_bq_project:
              componentInputParameter: output_bq_project
            pipelinechannel--output_bq_table:
              componentInputParameter: output_bq_table
            pipelinechannel--path_bucket:
              componentInputParameter: path_bucket
            pipelinechannel--prod_config:
              componentInputParameter: prod_config
            pipelinechannel--project:
              componentInputParameter: project
        taskInfo:
          name: exit-handler-1
      vertex-pipelines-notification-email:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-vertex-pipelines-notification-email
        dependentTasks:
        - exit-handler-1
        inputs:
          parameters:
            pipeline_task_final_status:
              taskFinalStatus:
                producerTask: exit-handler-1
            recipients:
              runtimeValue:
                constant:
                - jhon.efrainpr01@gmail.com
        taskInfo:
          name: vertex-pipelines-notification-email
        triggerPolicy:
          strategy: ALL_UPSTREAM_TASKS_COMPLETED
  inputDefinitions:
    parameters:
      labels:
        parameterType: STRUCT
      location:
        parameterType: STRING
      model_name:
        parameterType: STRING
      name_bucket:
        parameterType: STRING
      output_bq_dataset:
        parameterType: STRING
      output_bq_project:
        parameterType: STRING
      output_bq_table:
        parameterType: STRING
      path_bucket:
        parameterType: STRING
      prod_config:
        parameterType: STRUCT
      project:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
